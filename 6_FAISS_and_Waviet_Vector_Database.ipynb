{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AIWalaBro/GenerativeAI_Scratch_to_Advanced/blob/main/6_FAISS_and_Waviet_Vector_Database.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83RAjskJloUp",
        "outputId": "6c32ecf8-495d-4c4b-8d40-d235d992e421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.0.1)\n",
            "Requirement already satisfied: sentence-transformers==2.2.2 in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.37.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (2.2.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence-transformers==2.2.2) (12.4.99)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==2.2.2) (3.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers==2.2.2) (10.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain\n",
        "!pip install pypdf\n",
        "!pip install sentence-transformers==2.2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0fAVXcxNmdVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86ab83c5-3c90-4835-e6d1-ad3022ad7280"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.14.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir pdfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TE6V4xw7qXhV",
        "outputId": "e70cbed8-f797-47c8-e0fc-f7326410fbc1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘pdfs’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader"
      ],
      "metadata": {
        "id": "iWgansLBqloJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFDirectoryLoader(\"pdfs\")\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "7wwlAnI9qnvu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9YRWLQKqpg5",
        "outputId": "cb35be5d-bdd9-4add-edfc-d012ae0f4f7f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='CS391R: Robot Learning (Fall 2021)\\nYou Only Look Once (YOLO): Unified, Real-Time Object Detection\\n1Presenter: Shivang SinghSept 2nd, 2021', metadata={'source': 'pdfs/yolo.pdf', 'page': 0}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)2Problem Addressed: Object Detection❖Object detection is the problem of both locating ANDclassifying objects ❖Goal of YOLO algorithm is to do object detection both fast ANDwith high accuracy\\n“Deep Learning for Vision Systems” (Elgendy)Object Detection vs Classification', metadata={'source': 'pdfs/yolo.pdf', 'page': 1}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)3Importance of Object Detection for Robotics❖Visual modality is very powerful❖Humans are able to detect objects and do perception using just this modality in real time (not needing radar) ❖If we want responsive robot systems that work in real time (without specialized sensors) almost real time vision based object detection can help greatly\\nVision based vs LIDAR (self driving)\\nTesla Investor Day Presentation', metadata={'source': 'pdfs/yolo.pdf', 'page': 2}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)4Previous Object Detection ParadigmThis pipeline was used in nearly all SOTA Object Detection prior: \\nStep 1: Scan the image to generate candidate bounding boxes\\nImage ClassifierLabel + confidencehat -0.92racket -0.2ball -0.23Step 2: Run the bounding box through a classifierStep 3: Conduct post-processing (filtering out redundant bounding boxes)Diagram developed by presenter', metadata={'source': 'pdfs/yolo.pdf', 'page': 3}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)5Key Insights❖A separate model for generating bounding boxes and for classification (more complicated model pipeline)❖Need to run classification many times (expensive computation)❖Looks at limited part of the image (lacks contextual information for detection) Previous Approaches❖A single neural network for localization and for classification (less complicated pipeline)❖Need to inference only once (efficient computation)❖Looks at the entire image each time leading to less false positives (has contextual information for detection) YOLO algorithm', metadata={'source': 'pdfs/yolo.pdf', 'page': 4}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)6Formal Problem Setting❖Given an image generate bounding boxes, one for each detectable object in image ❖For each bounding box, output 5 predictions: x, y, w, h, confidence. Also output class❖x, y (coordinates for center of bounding box)❖w,h (width and height)❖confidence (probability bounding box has object)❖class (classification of object in bounding box)\\n', metadata={'source': 'pdfs/yolo.pdf', 'page': 5}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)7Related Work-R-CNN or Region Based Convolutional Network (Girshick et al. 2014):-Used the sliding window approach from earlier, with Selective Search, a smarter way to select candidates (which means there is less computation)-Still feeds a limited part of the image to the classifier-Drawbacks: Large pipeline, slow, too many false positives-Fast and Faster R-CNN: -Optimize parts of the pipeline described earlier -Drawbacks: loses accuracy-Deep Multibox (Szegedy et. al 2014):-Train a CNN to find areas of interest-Drawbacks: Doesn’t address classification only localization', metadata={'source': 'pdfs/yolo.pdf', 'page': 6}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)8Related Work-MultiGrasp (Redmon et. al 2014)-Similar to YOLO-A much simpler task (only needs to predict object not multiple objects)', metadata={'source': 'pdfs/yolo.pdf', 'page': 7}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)9YOLO overview❖First, image is split into a SxS grid❖For each grid square, generate B bounding boxes❖For each bounding box, there are 5 predictions: x, y, w, h, confidence \\nS = 3, B = 2', metadata={'source': 'pdfs/yolo.pdf', 'page': 8}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)\\n10YOLO Training❖YOLO is a regression algorithm. What is X? What is Y?❖X is simple, just an image width (in pixels) * height (in pixels) * RGB values❖Y is a tensor of size S * S * (B * 5 + C)❖B*5 + C term represents the predictions + class predicted distribution for a grid blockFor each grid block, we have a vector like this. For this example B is 2 and C is 2\\nGT label example:\\n', metadata={'source': 'pdfs/yolo.pdf', 'page': 9}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)11YOLO Architecture-Now that we know the input and output, we can discuss the model-We are given 448 by 448 by 3 as our input.-Implementation uses 7 convolution layers -Paper parameters: S = 7, B = 2, C = 20-Output is S*S*(5B+C) = 7*7*(5*2+20) = 7*7*30\\n', metadata={'source': 'pdfs/yolo.pdf', 'page': 10}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)12YOLO Prediction\\n❖We then use the output to make final detections❖Use a threshold to filter out bounding boxes with low P(Object)❖In order to know the class for the bounding box compute score take argmax over the distribution Pr(Class|Object) for the grid the bounding box’s center is in', metadata={'source': 'pdfs/yolo.pdf', 'page': 11}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)13Non-maximal suppression❖Most of the time objects fall in one grid, however it is still possible to get redundant boxes (rare case as object must be close to multiple grid cells for this to happen)❖Discard bounding box with high overlap (keeping the bounding box with highest confidence)❖Adds 2-3% on final mAP score\\n', metadata={'source': 'pdfs/yolo.pdf', 'page': 12}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)14YOLO Objective Function❖For YOLO, we need to minimize the following loss❖Sum squared error is used\\nCoordinate Loss: Minimize the difference between x,y,w,h pred and x,y,w,h ground truth.  ONLY IF object exists in grid box and if bounding box is resp for predClass loss, minimize loss between true class of object in grid box Confidence Loss: Loss based on confidence ONLY IF there is object No Object Loss based on confidence if there is no object', metadata={'source': 'pdfs/yolo.pdf', 'page': 13}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)15Experimental Setup❖Authors compare YOLO against the previous work described above on PASCAL VOC 2007, and VOC 2012 as well as out of domain art dataset ❖Correct if IOU metric above .5 and class is correct❖Use two performance metrics:➢mAP score: mean average precision➢FPS: frames per second❖Add FAST YOLO: which has less parameters\\n', metadata={'source': 'pdfs/yolo.pdf', 'page': 14}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)16Experimental Results\\n❖Baseline YOLO outperform real time detectors by large amount❖Do better than most less than real time as well ', metadata={'source': 'pdfs/yolo.pdf', 'page': 15}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)17\\nExperimental Results', metadata={'source': 'pdfs/yolo.pdf', 'page': 16}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)18Exper-Makes far less background errors (less likely to predict false positives on background)-IOU is VERY small with any ground truth label-But far more localization errors-Correct class, IOU is somewhat small\\nExperimental Results -Error Analysis\\nBackground errorLocalization error', metadata={'source': 'pdfs/yolo.pdf', 'page': 17}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)19❖Ran YOLO + competitors (trained on natural images) on art ❖Does well on artistic datasets where more having global context greatly helpsExperimental Results -Out of Domain\\n', metadata={'source': 'pdfs/yolo.pdf', 'page': 18}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)20Discussion of Results❖Pro: YOLO is a lot faster than the other algorithms for image detection❖Pro: YOLO’s use of global information rather than only local information allows it to understand contextual information when doing object detection➢Does better in domains such as artwork due to this❖Con: YOLO lagged behind the SOTA models in object detection➢This is attributed to making many localization errors and unable to detect small object', metadata={'source': 'pdfs/yolo.pdf', 'page': 19}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)21Critique / Limitations / Open Issues ❖Performance lags behind SOTA ❖Requires data to be labeled with bounding boxes, hard to collect for many classes➢Previous work could generalize better since it used image classifier➢2014 COCO dataset (very large dataset) addressed this somewhat❖Regarding experiments: number of classes predicted is very limited➢Not convinced that YOLO v1 is generalizable ❖Confidence output of YOLO not confidence of class but P(Object), lowers interpretability❖Another limitation of YOLO is that it imposed spatial constraints on the objects in the image since only B boxes can be predicted on an SxS grid❖Since the architecture only predicts boxes, this might make it less useful for irregular shapes ', metadata={'source': 'pdfs/yolo.pdf', 'page': 20}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)22Future Work for Paper / Reading❖One extension of this work would be to look at image segmentation and see if the insights carry over○YOLOACT (Boyla et al 2019): Real time image segmentation ❖YOLO has been upgraded 2 times ○Solves a lot of issues relating to detecting small objects, generalizability, and localization\\nYOLOACT example', metadata={'source': 'pdfs/yolo.pdf', 'page': 21}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)23Extended Readings❖YOLO v2 (https://arxiv.org/abs/1506.02640) (extends on the work greatly)(Redmond et al 2016)➢Deals with the generalizability problem, has 9000 classes➢Class probability distribution per bounding box, not per grid➢High resolution classifier (finetune on high resolution)➢Batch norm➢Trained on MSCOCO (released after YOLO v1 paper)❖YOLO v3 (https://arxiv.org/abs/1804.02767) ➢“Incremental Improvement”➢Uses independent logistic classifiers for class■Allows for more specificity in classes\\n', metadata={'source': 'pdfs/yolo.pdf', 'page': 22}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)24Summary❖Object detection is the problem of detecting multiple objects in an image❖Almost real time object detection can make highly responsive robot systems without complex sensors❖Prior work relies on a large architecture with numerous parts to optimize❖YOLO proposes a unified architecture, which does all the tasks in one model and by one inference over the entire image❖They show enormous speed improvement and show that they can beat most other prior work in terms of mAPs', metadata={'source': 'pdfs/yolo.pdf', 'page': 23})]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "4vi6LpMTqshP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
        "text_chunks = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "THMXHHHJqwq5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4YWi4G1qym3",
        "outputId": "3770c5e8-7042-4282-bed6-791e3f948679"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhMo15dsq0YP",
        "outputId": "b2ed0242-f3bc-4cc8-8a3f-eb500af74eb2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='CS391R: Robot Learning (Fall 2021)3Importance of Object Detection for Robotics❖Visual modality is very powerful❖Humans are able to detect objects and do perception using just this modality in real time (not needing radar) ❖If we want responsive robot systems that work in real time (without specialized sensors) almost real time vision based object detection can help greatly\\nVision based vs LIDAR (self driving)\\nTesla Investor Day Presentation', metadata={'source': 'pdfs/yolo.pdf', 'page': 2})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "Fvs-T22Vq2bH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "lUr498liq4QX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "3KUSmcUWq6Ho"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore=FAISS.from_documents(text_chunks, embeddings)"
      ],
      "metadata": {
        "id": "3pQ3ynVKq7vI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is correaltion?\""
      ],
      "metadata": {
        "id": "nNuelxqSq9mR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "EHDGBx2dtMFh",
        "outputId": "41863420-a196-4d04-a742-2a88e595c221"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is correaltion?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = vectorstore.similarity_search(query, k=3)"
      ],
      "metadata": {
        "id": "vgTDrnjltNRv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY-vI0-ltmTD",
        "outputId": "a9d0d30b-f1ad-4b6d-e4f3-ece91475e308"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='CS391R: Robot Learning (Fall 2021)\\n10YOLO Training❖YOLO is a regression algorithm. What is X? What is Y?❖X is simple, just an image width (in pixels) * height (in pixels) * RGB values❖Y is a tensor of size S * S * (B * 5 + C)❖B*5 + C term represents the predictions + class predicted distribution for a grid blockFor each grid block, we have a vector like this. For this example B is 2 and C is 2\\nGT label example:', metadata={'source': 'pdfs/yolo.pdf', 'page': 9}),\n",
              " Document(page_content='other prior work in terms of mAPs', metadata={'source': 'pdfs/yolo.pdf', 'page': 23}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)20Discussion of Results❖Pro: YOLO is a lot faster than the other algorithms for image detection❖Pro: YOLO’s use of global information rather than only local information allows it to understand contextual information when doing object detection➢Does better in domains such as artwork due to this❖Con: YOLO lagged behind the SOTA models in object detection➢This is attributed to making many localization errors and unable to detect small object', metadata={'source': 'pdfs/yolo.pdf', 'page': 19})]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erkS3zq_top3",
        "outputId": "41fd80a5-f67a-4aa4-94b7-e4c53e586421"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "j6SRJcbEtrey"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "Yn4oSAy3tuHK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "Cz18y89Wt8W6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "titCze69t_il",
        "outputId": "98124f0d-1654-480c-d207-6fbdfd9a361d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "HOfmOEBkuCE8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())"
      ],
      "metadata": {
        "id": "WSBkTjhAuEj3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is correaltion?\""
      ],
      "metadata": {
        "id": "pWb-Pj2muG84"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(qa.run(query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hz3kODcauLlQ",
        "outputId": "8bac08c3-62a2-4e1d-c69a-19d715cb8868"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Correlation is a measure of the relationship between two variables. It indicates how closely related two variables are, and whether they tend to move in the same direction or opposite directions. A correlation coefficient is a statistical measure that quantifies the strength and direction of the relationship between two variables. It ranges from -1 to 1, with a positive correlation indicating a direct relationship and a negative correlation indicating an inverse relationship.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QuL7cUQEuOB0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WAVIET CLOUD VECTOR DATABASE"
      ],
      "metadata": {
        "id": "zL0Dx4O3zHug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install weaviate-client\n",
        "!pip install langchain\n",
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOfyalMRzL7C",
        "outputId": "038fe13e-53a4-4804-8954-73286bc7efcd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: weaviate-client in /usr/local/lib/python3.10/dist-packages (4.5.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.30.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (2.31.0)\n",
            "Requirement already satisfied: httpx==0.27.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (0.27.0)\n",
            "Requirement already satisfied: validators==0.22.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (0.22.0)\n",
            "Requirement already satisfied: authlib<2.0.0,>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (1.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (2.6.4)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.57.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (1.62.1)\n",
            "Requirement already satisfied: grpcio-tools<2.0.0,>=1.57.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (1.62.1)\n",
            "Requirement already satisfied: grpcio-health-checking<2.0.0,>=1.57.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (1.62.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx==0.27.0->weaviate-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.27.0->weaviate-client) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.27.0->weaviate-client) (1.0.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx==0.27.0->weaviate-client) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.27.0->weaviate-client) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx==0.27.0->weaviate-client) (0.14.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (from authlib<2.0.0,>=1.2.1->weaviate-client) (42.0.2)\n",
            "Requirement already satisfied: protobuf>=4.21.6 in /usr/local/lib/python3.10/dist-packages (from grpcio-health-checking<2.0.0,>=1.57.0->weaviate-client) (4.23.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools<2.0.0,>=1.57.0->weaviate-client) (67.7.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.0->weaviate-client) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.0->weaviate-client) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.0->weaviate-client) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.30.0->weaviate-client) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.30.0->weaviate-client) (1.26.18)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx==0.27.0->weaviate-client) (1.2.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography->authlib<2.0.0,>=1.2.1->weaviate-client) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography->authlib<2.0.0,>=1.2.1->weaviate-client) (2.21)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.12)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.28)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.28 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.28)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.31 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.32)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.27)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.31->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.31->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.9.15)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.14.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "WKVcXBjezaGJ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "-ncISzJizwcG"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WEAVIATE_API_KEY = \"YOUR API KEY\"\n",
        "WEAVIATE_CLUSTER = \"YOUR CLUSTER URL\""
      ],
      "metadata": {
        "id": "CUGcZCqmzzDQ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install unstructured\n",
        "# !pip install \"unstructured[pdf]\""
      ],
      "metadata": {
        "id": "67dgJfJgz87w"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Reading"
      ],
      "metadata": {
        "id": "JOZWW2tx0uqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4is7T6Xz_gH",
        "outputId": "279aafea-4a76-44a0-98eb-682a7b186057"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "\n",
        "loader = PyPDFDirectoryLoader(\"pdfs\")\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "NeIarV980mL6"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcePWNTc0oyC",
        "outputId": "5b690a6f-c87b-4fbe-8e2c-369564d5cd31"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='CS391R: Robot Learning (Fall 2021)\\nYou Only Look Once (YOLO): Unified, Real-Time Object Detection\\n1Presenter: Shivang SinghSept 2nd, 2021', metadata={'source': 'pdfs/yolo.pdf', 'page': 0}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)2Problem Addressed: Object Detection❖Object detection is the problem of both locating ANDclassifying objects ❖Goal of YOLO algorithm is to do object detection both fast ANDwith high accuracy\\n“Deep Learning for Vision Systems” (Elgendy)Object Detection vs Classification', metadata={'source': 'pdfs/yolo.pdf', 'page': 1}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)3Importance of Object Detection for Robotics❖Visual modality is very powerful❖Humans are able to detect objects and do perception using just this modality in real time (not needing radar) ❖If we want responsive robot systems that work in real time (without specialized sensors) almost real time vision based object detection can help greatly\\nVision based vs LIDAR (self driving)\\nTesla Investor Day Presentation', metadata={'source': 'pdfs/yolo.pdf', 'page': 2}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)4Previous Object Detection ParadigmThis pipeline was used in nearly all SOTA Object Detection prior: \\nStep 1: Scan the image to generate candidate bounding boxes\\nImage ClassifierLabel + confidencehat -0.92racket -0.2ball -0.23Step 2: Run the bounding box through a classifierStep 3: Conduct post-processing (filtering out redundant bounding boxes)Diagram developed by presenter', metadata={'source': 'pdfs/yolo.pdf', 'page': 3}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)5Key Insights❖A separate model for generating bounding boxes and for classification (more complicated model pipeline)❖Need to run classification many times (expensive computation)❖Looks at limited part of the image (lacks contextual information for detection) Previous Approaches❖A single neural network for localization and for classification (less complicated pipeline)❖Need to inference only once (efficient computation)❖Looks at the entire image each time leading to less false positives (has contextual information for detection) YOLO algorithm', metadata={'source': 'pdfs/yolo.pdf', 'page': 4}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)6Formal Problem Setting❖Given an image generate bounding boxes, one for each detectable object in image ❖For each bounding box, output 5 predictions: x, y, w, h, confidence. Also output class❖x, y (coordinates for center of bounding box)❖w,h (width and height)❖confidence (probability bounding box has object)❖class (classification of object in bounding box)\\n', metadata={'source': 'pdfs/yolo.pdf', 'page': 5}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)7Related Work-R-CNN or Region Based Convolutional Network (Girshick et al. 2014):-Used the sliding window approach from earlier, with Selective Search, a smarter way to select candidates (which means there is less computation)-Still feeds a limited part of the image to the classifier-Drawbacks: Large pipeline, slow, too many false positives-Fast and Faster R-CNN: -Optimize parts of the pipeline described earlier -Drawbacks: loses accuracy-Deep Multibox (Szegedy et. al 2014):-Train a CNN to find areas of interest-Drawbacks: Doesn’t address classification only localization', metadata={'source': 'pdfs/yolo.pdf', 'page': 6}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)8Related Work-MultiGrasp (Redmon et. al 2014)-Similar to YOLO-A much simpler task (only needs to predict object not multiple objects)', metadata={'source': 'pdfs/yolo.pdf', 'page': 7}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)9YOLO overview❖First, image is split into a SxS grid❖For each grid square, generate B bounding boxes❖For each bounding box, there are 5 predictions: x, y, w, h, confidence \\nS = 3, B = 2', metadata={'source': 'pdfs/yolo.pdf', 'page': 8}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)\\n10YOLO Training❖YOLO is a regression algorithm. What is X? What is Y?❖X is simple, just an image width (in pixels) * height (in pixels) * RGB values❖Y is a tensor of size S * S * (B * 5 + C)❖B*5 + C term represents the predictions + class predicted distribution for a grid blockFor each grid block, we have a vector like this. For this example B is 2 and C is 2\\nGT label example:\\n', metadata={'source': 'pdfs/yolo.pdf', 'page': 9}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)11YOLO Architecture-Now that we know the input and output, we can discuss the model-We are given 448 by 448 by 3 as our input.-Implementation uses 7 convolution layers -Paper parameters: S = 7, B = 2, C = 20-Output is S*S*(5B+C) = 7*7*(5*2+20) = 7*7*30\\n', metadata={'source': 'pdfs/yolo.pdf', 'page': 10}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)12YOLO Prediction\\n❖We then use the output to make final detections❖Use a threshold to filter out bounding boxes with low P(Object)❖In order to know the class for the bounding box compute score take argmax over the distribution Pr(Class|Object) for the grid the bounding box’s center is in', metadata={'source': 'pdfs/yolo.pdf', 'page': 11}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)13Non-maximal suppression❖Most of the time objects fall in one grid, however it is still possible to get redundant boxes (rare case as object must be close to multiple grid cells for this to happen)❖Discard bounding box with high overlap (keeping the bounding box with highest confidence)❖Adds 2-3% on final mAP score\\n', metadata={'source': 'pdfs/yolo.pdf', 'page': 12}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)14YOLO Objective Function❖For YOLO, we need to minimize the following loss❖Sum squared error is used\\nCoordinate Loss: Minimize the difference between x,y,w,h pred and x,y,w,h ground truth.  ONLY IF object exists in grid box and if bounding box is resp for predClass loss, minimize loss between true class of object in grid box Confidence Loss: Loss based on confidence ONLY IF there is object No Object Loss based on confidence if there is no object', metadata={'source': 'pdfs/yolo.pdf', 'page': 13}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)15Experimental Setup❖Authors compare YOLO against the previous work described above on PASCAL VOC 2007, and VOC 2012 as well as out of domain art dataset ❖Correct if IOU metric above .5 and class is correct❖Use two performance metrics:➢mAP score: mean average precision➢FPS: frames per second❖Add FAST YOLO: which has less parameters\\n', metadata={'source': 'pdfs/yolo.pdf', 'page': 14}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)16Experimental Results\\n❖Baseline YOLO outperform real time detectors by large amount❖Do better than most less than real time as well ', metadata={'source': 'pdfs/yolo.pdf', 'page': 15}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)17\\nExperimental Results', metadata={'source': 'pdfs/yolo.pdf', 'page': 16}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)18Exper-Makes far less background errors (less likely to predict false positives on background)-IOU is VERY small with any ground truth label-But far more localization errors-Correct class, IOU is somewhat small\\nExperimental Results -Error Analysis\\nBackground errorLocalization error', metadata={'source': 'pdfs/yolo.pdf', 'page': 17}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)19❖Ran YOLO + competitors (trained on natural images) on art ❖Does well on artistic datasets where more having global context greatly helpsExperimental Results -Out of Domain\\n', metadata={'source': 'pdfs/yolo.pdf', 'page': 18}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)20Discussion of Results❖Pro: YOLO is a lot faster than the other algorithms for image detection❖Pro: YOLO’s use of global information rather than only local information allows it to understand contextual information when doing object detection➢Does better in domains such as artwork due to this❖Con: YOLO lagged behind the SOTA models in object detection➢This is attributed to making many localization errors and unable to detect small object', metadata={'source': 'pdfs/yolo.pdf', 'page': 19}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)21Critique / Limitations / Open Issues ❖Performance lags behind SOTA ❖Requires data to be labeled with bounding boxes, hard to collect for many classes➢Previous work could generalize better since it used image classifier➢2014 COCO dataset (very large dataset) addressed this somewhat❖Regarding experiments: number of classes predicted is very limited➢Not convinced that YOLO v1 is generalizable ❖Confidence output of YOLO not confidence of class but P(Object), lowers interpretability❖Another limitation of YOLO is that it imposed spatial constraints on the objects in the image since only B boxes can be predicted on an SxS grid❖Since the architecture only predicts boxes, this might make it less useful for irregular shapes ', metadata={'source': 'pdfs/yolo.pdf', 'page': 20}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)22Future Work for Paper / Reading❖One extension of this work would be to look at image segmentation and see if the insights carry over○YOLOACT (Boyla et al 2019): Real time image segmentation ❖YOLO has been upgraded 2 times ○Solves a lot of issues relating to detecting small objects, generalizability, and localization\\nYOLOACT example', metadata={'source': 'pdfs/yolo.pdf', 'page': 21}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)23Extended Readings❖YOLO v2 (https://arxiv.org/abs/1506.02640) (extends on the work greatly)(Redmond et al 2016)➢Deals with the generalizability problem, has 9000 classes➢Class probability distribution per bounding box, not per grid➢High resolution classifier (finetune on high resolution)➢Batch norm➢Trained on MSCOCO (released after YOLO v1 paper)❖YOLO v3 (https://arxiv.org/abs/1804.02767) ➢“Incremental Improvement”➢Uses independent logistic classifiers for class■Allows for more specificity in classes\\n', metadata={'source': 'pdfs/yolo.pdf', 'page': 22}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)24Summary❖Object detection is the problem of detecting multiple objects in an image❖Almost real time object detection can make highly responsive robot systems without complex sensors❖Prior work relies on a large architecture with numerous parts to optimize❖YOLO proposes a unified architecture, which does all the tasks in one model and by one inference over the entire image❖They show enormous speed improvement and show that they can beat most other prior work in terms of mAPs', metadata={'source': 'pdfs/yolo.pdf', 'page': 23})]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Splitting"
      ],
      "metadata": {
        "id": "QdVWbasX0qy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "docs = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "vKOxSdF-0smK"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULyK8CUt2W_3",
        "outputId": "178126f0-ab0a-4b36-8bd7-c00e74ae3834"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding Convertion"
      ],
      "metadata": {
        "id": "etzof44l2cB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iHzTOac2ZNv",
        "outputId": "423b0955-9c13-4770-96e4-c32663b8113a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0PdhXXO2e1n",
        "outputId": "8be8b86d-1dac-4435-bdc6-e08c4dce0bbb"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x7eca977bffd0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x7eca977bfe20>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector Database Storage"
      ],
      "metadata": {
        "id": "cB6NehcE2pwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import weaviate\n",
        "from langchain.vectorstores import Weaviate\n",
        "\n",
        "#Connect to weaviate Cluster\n",
        "auth_config = weaviate.auth.AuthApiKey(api_key = WEAVIATE_API_KEY)\n",
        "WEAVIATE_URL = WEAVIATE_CLUSTER\n",
        "\n",
        "client = weaviate.Client(\n",
        "    url = WEAVIATE_URL,\n",
        "    additional_headers = {\"X-OpenAI-Api-key\": OPENAI_API_KEY},\n",
        "    auth_client_secret = auth_config,\n",
        "    startup_period = 10\n",
        ")"
      ],
      "metadata": {
        "id": "KyheE6Ei2nJG"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client.is_ready()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2g5_F_Y3ENv",
        "outputId": "39844015-f995-4950-a624-aa3852fd8cde"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define input structure\n",
        "client.schema.delete_all()\n",
        "client.schema.get()\n",
        "schema = {\n",
        "    \"classes\": [\n",
        "        {\n",
        "            \"class\": \"Chatbot\",\n",
        "            \"description\": \"Documents for chatbot\",\n",
        "            \"vectorizer\": \"text2vec-openai\",\n",
        "            \"moduleConfig\": {\"text2vec-openai\": {\"model\": \"ada\", \"type\": \"text\"}},\n",
        "            \"properties\": [\n",
        "                {\n",
        "                    \"dataType\": [\"text\"],\n",
        "                    \"description\": \"The content of the paragraph\",\n",
        "                    \"moduleConfig\": {\n",
        "                        \"text2vec-openai\": {\n",
        "                            \"skip\": False,\n",
        "                            \"vectorizePropertyName\": False,\n",
        "                        }\n",
        "                    },\n",
        "                    \"name\": \"content\",\n",
        "                },\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "}\n",
        "\n",
        "client.schema.create(schema)\n",
        "vectorstore = Weaviate(client, \"Chatbot\", \"content\", attributes=[\"source\"])"
      ],
      "metadata": {
        "id": "pZi5VbFo3IOP"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load text into the vectorstore\n",
        "text_meta_pair = [(doc.page_content, doc.metadata) for doc in docs]\n",
        "texts, meta = list(zip(*text_meta_pair))\n",
        "vectorstore.add_texts(texts, meta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhMhjBdJ3LWa",
        "outputId": "4c747fcb-a751-4b1a-c818-6ddeba7964af"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'error': [{'message': 'update vector: connection to: OpenAI API failed with status: 429 error: Rate limit reached for text-embedding-ada-002 in organization org-3jQeBcduPsYliZjTqmsVadY5 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.'}]}\n",
            "{'error': [{'message': 'update vector: connection to: OpenAI API failed with status: 429 error: Rate limit reached for text-embedding-ada-002 in organization org-3jQeBcduPsYliZjTqmsVadY5 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.'}]}\n",
            "{'error': [{'message': 'update vector: connection to: OpenAI API failed with status: 429 error: Rate limit reached for text-embedding-ada-002 in organization org-3jQeBcduPsYliZjTqmsVadY5 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.'}]}\n",
            "{'error': [{'message': 'update vector: connection to: OpenAI API failed with status: 429 error: Rate limit reached for text-embedding-ada-002 in organization org-3jQeBcduPsYliZjTqmsVadY5 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.'}]}\n",
            "{'error': [{'message': 'update vector: connection to: OpenAI API failed with status: 429 error: Rate limit reached for text-embedding-ada-002 in organization org-3jQeBcduPsYliZjTqmsVadY5 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.'}]}\n",
            "{'error': [{'message': 'update vector: connection to: OpenAI API failed with status: 429 error: Rate limit reached for text-embedding-ada-002 in organization org-3jQeBcduPsYliZjTqmsVadY5 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.'}]}\n",
            "{'error': [{'message': 'update vector: connection to: OpenAI API failed with status: 429 error: Rate limit reached for text-embedding-ada-002 in organization org-3jQeBcduPsYliZjTqmsVadY5 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.'}]}\n",
            "{'error': [{'message': 'update vector: connection to: OpenAI API failed with status: 429 error: Rate limit reached for text-embedding-ada-002 in organization org-3jQeBcduPsYliZjTqmsVadY5 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.'}]}\n",
            "{'error': [{'message': 'update vector: connection to: OpenAI API failed with status: 429 error: Rate limit reached for text-embedding-ada-002 in organization org-3jQeBcduPsYliZjTqmsVadY5 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.'}]}\n",
            "{'error': [{'message': 'update vector: connection to: OpenAI API failed with status: 429 error: Rate limit reached for text-embedding-ada-002 in organization org-3jQeBcduPsYliZjTqmsVadY5 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.'}]}\n",
            "{'error': [{'message': 'update vector: connection to: OpenAI API failed with status: 429 error: Rate limit reached for text-embedding-ada-002 in organization org-3jQeBcduPsYliZjTqmsVadY5 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.'}]}\n",
            "{'error': [{'message': 'update vector: connection to: OpenAI API failed with status: 429 error: Rate limit reached for text-embedding-ada-002 in organization org-3jQeBcduPsYliZjTqmsVadY5 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.'}]}\n",
            "{'error': [{'message': 'update vector: connection to: OpenAI API failed with status: 429 error: Rate limit reached for text-embedding-ada-002 in organization org-3jQeBcduPsYliZjTqmsVadY5 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.'}]}\n",
            "{'error': [{'message': 'update vector: connection to: OpenAI API failed with status: 429 error: Rate limit reached for text-embedding-ada-002 in organization org-3jQeBcduPsYliZjTqmsVadY5 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.'}]}\n",
            "{'error': [{'message': 'update vector: connection to: OpenAI API failed with status: 429 error: Rate limit reached for text-embedding-ada-002 in organization org-3jQeBcduPsYliZjTqmsVadY5 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.'}]}\n",
            "{'error': [{'message': 'update vector: connection to: OpenAI API failed with status: 429 error: Rate limit reached for text-embedding-ada-002 in organization org-3jQeBcduPsYliZjTqmsVadY5 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.'}]}\n",
            "{'error': [{'message': 'update vector: connection to: OpenAI API failed with status: 429 error: Rate limit reached for text-embedding-ada-002 in organization org-3jQeBcduPsYliZjTqmsVadY5 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.'}]}\n",
            "{'error': [{'message': 'update vector: connection to: OpenAI API failed with status: 429 error: Rate limit reached for text-embedding-ada-002 in organization org-3jQeBcduPsYliZjTqmsVadY5 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.'}]}\n",
            "{'error': [{'message': 'update vector: connection to: OpenAI API failed with status: 429 error: Rate limit reached for text-embedding-ada-002 in organization org-3jQeBcduPsYliZjTqmsVadY5 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.'}]}\n",
            "{'error': [{'message': 'update vector: connection to: OpenAI API failed with status: 429 error: Rate limit reached for text-embedding-ada-002 in organization org-3jQeBcduPsYliZjTqmsVadY5 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.'}]}\n",
            "{'error': [{'message': 'update vector: connection to: OpenAI API failed with status: 429 error: Rate limit reached for text-embedding-ada-002 in organization org-3jQeBcduPsYliZjTqmsVadY5 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.'}]}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['894cba1f-8fa9-4594-bbfd-e0abcaede76e',\n",
              " '1511e09f-284f-4b10-93fe-97ccf7d8f9a5',\n",
              " 'a3acdf22-60a8-436e-bd8f-013a7e2b8add',\n",
              " '262bec72-bd6a-46e7-86f1-55b5cb341bd2',\n",
              " 'b5a0791c-5dbb-4b92-a76b-5b34e7cfd58a',\n",
              " '5e2a1ec4-3131-47a1-8fa2-1758cb4d4684',\n",
              " 'e9c25161-608b-4761-b853-b1432fee82bd',\n",
              " '1cdd2754-56cf-42e7-88e2-31348a82b69d',\n",
              " 'b9a2ffb7-6860-4a65-b560-f95914f70a05',\n",
              " '6a53b2ff-fbaa-4fcd-a4ac-4ae1ae775895',\n",
              " 'a02b0e89-358c-4d7f-bce9-5923c168c715',\n",
              " '7c9c6198-a49c-4bf5-ab61-e5b198f181ed',\n",
              " '1db699c4-a0e0-4181-8348-2616a4dc3ada',\n",
              " '8c64ccef-97aa-456b-b805-d561723aef32',\n",
              " '432b36cc-01d3-416a-855c-769d1c027089',\n",
              " 'f7a083db-b3fd-432b-b075-b7c414e0c33c',\n",
              " '798dee7f-413f-4bd6-b684-1df6e1dff8c0',\n",
              " '72bcf156-9386-4d19-a2c6-a4c3471ac39a',\n",
              " 'e8490b36-8c6d-4e60-b76d-15ce05830ca3',\n",
              " 'd1ff452a-dd02-43f8-8728-feb2ab5a676d',\n",
              " 'babaef82-300e-4584-8217-fd44bd9d077d',\n",
              " '8a111394-2922-4a8f-b8cd-a151f32ad005',\n",
              " 'b73974bc-ec3f-4977-8a55-f267a053da36',\n",
              " '6dba5189-517f-4e82-986e-4eb2974e66c1']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is a correlation?\"\n",
        "\n",
        "# retrieve text related to the query\n",
        "docs = vectorstore.similarity_search(query, top_k=3)"
      ],
      "metadata": {
        "id": "cmG6ECtZ3T3e"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD4H_PVH3cNR",
        "outputId": "ae3ab238-44e6-4823-e9a5-ba08ccc54f21"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='CS391R: Robot Learning (Fall 2021)8Related Work-MultiGrasp (Redmon et. al 2014)-Similar to YOLO-A much simpler task (only needs to predict object not multiple objects)', metadata={'source': 'pdfs/yolo.pdf'}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)24Summary❖Object detection is the problem of detecting multiple objects in an image❖Almost real time object detection can make highly responsive robot systems without complex sensors❖Prior work relies on a large architecture with numerous parts to optimize❖YOLO proposes a unified architecture, which does all the tasks in one model and by one inference over the entire image❖They show enormous speed improvement and show that they can beat most other prior work in terms of mAPs', metadata={'source': 'pdfs/yolo.pdf'}),\n",
              " Document(page_content='CS391R: Robot Learning (Fall 2021)21Critique / Limitations / Open Issues ❖Performance lags behind SOTA ❖Requires data to be labeled with bounding boxes, hard to collect for many classes➢Previous work could generalize better since it used image classifier➢2014 COCO dataset (very large dataset) addressed this somewhat❖Regarding experiments: number of classes predicted is very limited➢Not convinced that YOLO v1 is generalizable ❖Confidence output of YOLO not confidence of class but P(Object), lowers interpretability❖Another limitation of YOLO is that it imposed spatial constraints on the objects in the image since only B boxes can be predicted on an SxS grid❖Since the architecture only predicts boxes, this might make it less useful for irregular shapes', metadata={'source': 'pdfs/yolo.pdf'})]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom chatbot"
      ],
      "metadata": {
        "id": "lbLy9Nwj3h19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "LOveswrP3ewM"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define chain\n",
        "chain = load_qa_chain(\n",
        "    OpenAI(),\n",
        "    chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "eTu4CG5f3kb1"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create answer\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "b_2FiK3U3nKA",
        "outputId": "8461ce30-1a5a-4dd5-a3ae-9dace741522d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Correlation is a statistical measure that shows the relationship or association between two variables. It indicates how much and in what way one variable changes with respect to the other variable. A positive correlation means that as one variable increases, the other variable also increases, while a negative correlation means that as one variable increases, the other variable decreases. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yIAIs7A93p2f"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoVO4dyreuSQTheGLwKzze",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}